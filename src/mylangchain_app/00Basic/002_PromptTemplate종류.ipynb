{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_4\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate 의 from_template() 함수 사용\n",
    "* 주로 LLM(텍스트 완성형 모델, ex. Ollama, GPT-3.5)과 함께 사용\n",
    "* 하나의 문자열 프롬프트를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPT는 인터넷 글을 방대하게 수집해 토큰 단위로 자른 뒤, 다음 토큰을 맞히는 ‘예측 게임’을 반복하며 스스로 패턴을 '\n",
      " '학습합니다.  \\n'\n",
      " '학습이 끝나면 주어진 질문을 토큰 시퀀스로 받아, 각 위치에서 가장 적절한 단어의 확률을 계산해 한 토큰씩 순차적으로 생성합니다.  \\n'\n",
      " '이렇게 얻은 확률 분포를 바탕으로 사람이 이해할 만한 자연스러운 문장을 만들어내는 것이 핵심 원리입니다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    #model=\"openai/gpt-oss-120b\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate 결합하기\n",
    "* 동일한 Prompt 패턴을 사용하지만 여러 개의 질문을 작성해서 LLM을 실행할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['count', 'language', 'model_name'] input_types={} partial_variables={} template='{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.'\n",
      "('ChatGPT는 인터넷의 방대한 텍스트를 학습해 단어 확률을 익힌 뒤, 사람이 준 프롬프트에 가장 자연스러운 다음 말을 예측·생성하는 '\n",
      " '방식으로 작동합니다.  \\n'\n",
      " '학습 과정에선 인간의 대화 데이터로 미세 조정(강화학습·인간 피드백)을 거쳐 사실성·윤리성을 높입니다.  \\n'\n",
      " '결과적으로 주어진 문맥을 기억해 논리적이고 유창한 응답을 이어가는 대화형 언어 모델이 됩니다.\\n'\n",
      " '\\n'\n",
      " 'ChatGPT의 주요 장점  \\n'\n",
      " '- 다양한 주제에 대한 이해도와 유창한 문장 생성  \\n'\n",
      " '- 질문·번역·요약·코드 작성 등 다양한 업무를 하나의 모델로 처리  \\n'\n",
      " '- 대화 맥락을 유지해 자연스러운 연속 대화 가능  \\n'\n",
      " '- 사용자 피드백을 반영해 지속적으로 개선되며, 즉각적이고 24시간 이용 가능\\n'\n",
      " '\\n'\n",
      " 'ChatGPT와 비슷한 한국어 AI 모델: 구글 바드(제미나이), 메타 람마, 에이디티 누스(Anthropic 클라우드), 카카오 코랑, '\n",
      " '네이버 클로바X, 카카오브레인 코기(1.0/2.0) 등')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"한국어\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"한국어\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate 의 파라미터를 배열 형태로 하여 여러개 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.', 'Gemini 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.', 'claude 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.']\n",
      "<class 'str'> GPT-4 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.\n",
      "('GPT-4는 대규모 텍스트 데이터를 바탕으로 다음 단어를 예측하도록 사전 학습된 트랜스포머 기반 언어 모델입니다.  \\n'\n",
      " '지도 학습 방식으로 인간이 작성한 프롬프트-응답 쌍을 반복해서 학습해, 사람이 선호하는 답변 패턴을 점점 모방합니다.  \\n'\n",
      " '강화학습과 인간 피드백(RLHF)을 추가로 거치면서 안전성·유용성·정직성을 높인 최종 모델이 완성됩니다.')\n",
      "<class 'str'> Gemini 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.\n",
      "('Gemini는 대규모 텍스트·이미지·오디오 등 다중 모달 데이터를 동시에 처리하도록 설계된 전이 학습 기반의 거대 신경망 모델이다.  \\n'\n",
      " '먼저 인터넷 긁기, 책 스캔, 유튜브 자막 등 방대한 공개 데이터를 바탕으로 다음 토큰을 예측하는 일반 사전 학습을 수행한다.  \\n'\n",
      " '이후 인간의 선호에 맞춰 답변 품질을 높이도록 강화학습·인간 피드백(RLHF)을 반복해 미세 조정하며, Chain-of-Thought '\n",
      " '프롬프트를 통해 다단계 추론 능력도 강화한다.  \\n'\n",
      " '결과적으로 Gemini는 텍스트 생성, 코드 작성, 이미지 분석, 수학·과학 문제 풀이 등 다양한 작업을 단일 모델로 처리할 수 있게 '\n",
      " '된다.')\n",
      "<class 'str'> claude 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.\n",
      "('Claude는 방대한 텍스트를 통해 다음 토큰을 예측하도록 사전학습되고, 인간 대화 샘플로 강화학습·인간 피드백(RLHF)으로 가치를 '\n",
      " '맞춘다.  \\n'\n",
      " \"이어서 '헌법(Constitutional AI)'이라는 원칙 집합을 스스로 참고하게 하여 유해·편향 답변을 줄이고, 반복적 자기개선으로 \"\n",
      " '정정 능력을 키운다.  \\n'\n",
      " '모델은 Transformer 아키텍처 기반으로, 주의 메커니즘이 문맥의 중요 부분에 집중하며 연쇄 추론을 수행한다.  \\n'\n",
      " '추론 시에는 대화 맥락을 계속 유지하고, 필요에 따라 사실 확인·외부 도구 호출을 통해 정확성과 유용성을 높인다.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 3},\n",
    "    {\"model_name\": \"Gemini\", \"count\": 4},\n",
    "    {\"model_name\": \"claude\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "# 각 딕셔너리 q를 언패킹해 .format의 field를 채움\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple 형태의 system, user, assistant 메시지 지원\n",
    "* 여러 개의 메시지를 조합하여 LLM에게 전달 가능\n",
    "* 간결성과 가독성이 높고 단순한 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='This system is an expert in answering questions about AI.      Please provide clear and detailed explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ChatGPT 모델의 학습 원리를 설명해 주세요.', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ChatGPT는 “GPT(Generative Pre-trained Transformer)” 계열 모델의 한 버전으로,  \n",
      "“**미리 학습(pre-train) → 미세 조정(fine-tune)**”이라는 2단계 흐름을 기반으로 만들어집니다.  \n",
      "핵심 원리는 **“주어진 앞 단어들을 보고 다음 단어를 예측하는(=확률을 계산하는) 방대한 자기 회귀(self-supervised) 언어 모델”**입니다.  \n",
      "이 과정을 단계별로 풀어 보면 다음과 같습니다.\n",
      "\n",
      "--------------------------------------------------\n",
      "1. 자기 지도 학습(Self-Supervised Learning) : Pre-training\n",
      "--------------------------------------------------\n",
      "1) 데이터  \n",
      "   - 공개 웹(웹페이지, 위키, 책, 깃헙 등)에서 수 TB 급의 텍스트  \n",
      "   - 별도로 “정답”을 만들 필요 없음 → 텍스트 자체가 레이블이 됨\n",
      "\n",
      "2) 학습 목표  \n",
      "   - 입력 시퀀스의 다음 토큰(token, 단어·부분 단어·기호)을 맞추는 최대 우도(MLL, Maximum Likelihood)  \n",
      "   - 수식화 : max Σ log P(wᵢ | w₁…wᵢ₋₁; θ)  \n",
      "   - 결과적으로 웬만한 지식·문법·상식이 모델 파라미터(θ)에 압축됨\n",
      "\n",
      "3) 기술적 요소  \n",
      "   - Transformer 디코더 구조  \n",
      "     • Multi-Head Self-Attention : 긴 거리 의존성을 한 번에 포착  \n",
      "     • Masked Attention : 미래 토큰을 못 보게 하여 자기 회귀 조건 만족  \n",
      "   - 파라미터 수 : 억~천억 개(버전에 따라 다름)  \n",
      "   - 분산 학습 : 수천 개 GPU/TPU, 수 주~수월 소요\n",
      "\n",
      "--------------------------------------------------\n",
      "2. 지도·강화 학습 : Fine-tuning & Alignment\n",
      "--------------------------------------------------\n",
      "1) Supervised Fine-Tuning(SFT, 지도 미세조정)  \n",
      "   - (질문, 적절한 답변) 쌍 1~10만 개 정도를 사람이 직접 작성  \n",
      "   - “대화” 형식으로 학습 → ChatGPT처럼 “대화”를 이어가는 능력 확보\n",
      "\n",
      "2) Reward Model 학습(RM)  \n",
      "   - 같은 프롬프트에 대한 여러 답변을 비교·순위 매김(사람 표시)  \n",
      "   - 이 비교 데이터로 “어떤 답변이 더 나은가”를 예측하는 별도 신경망 RM을 훈련\n",
      "\n",
      "3) 강화 학습(RLHF, Reinforcement Learning from Human Feedback)  \n",
      "   - PPO(Proximal Policy Optimization) 알고리즘 사용  \n",
      "   - 정책(policy) = 지금의 ChatGPT, 보상(reward) = RM 출력값  \n",
      "   - 보상을 높이도록 파라미터 업데이트 → ‘사람이 선호하는’ 응답 확률 증가  \n",
      "   - 동시에 KL 제약을 걸어 원본 모델과 너무 멀어지지 않도록 안정성 확보\n",
      "\n",
      "--------------------------------------------------\n",
      "3. 생성(인퍼런스) 원리\n",
      "--------------------------------------------------\n",
      "1) 입력(프롬프트) → 토큰 시퀀스로 변환  \n",
      "2) 모델은 이전 토큰들만 보고 다음 토큰 확률 계산  \n",
      "3) 확률 분포에서 토큰 하나 샘플링  \n",
      "   - greedy, top-k, top-p(= nucleus) 등 다양한 샘플링 전략  \n",
      "4) 샘플링한 토큰을 다시 입력에 추가하고 2~3 반복 → EOS 토큰이 나올 때까지  \n",
      "5) 토큰 → 문자열 복원 → 사용자에게 출력\n",
      "\n",
      "--------------------------------------------------\n",
      "4. “ChatGPT가 질문에 답할 수 있는 이유” 요약\n",
      "--------------------------------------------------\n",
      "- 대규모 텍스트로 “세상 지식 + 언어 문법”을 학습한 뒤  \n",
      "- 사람이 좋아할 만한 대화 스타일로 방향 튜닝(Alignment)했기 때문에  \n",
      "- 주어진 맥락만으로 다음에 옳을 법한 단어를 확률적으로 선택하면서  \n",
      "- 논리적·문법적으로 자연스러운 문장을 만들어 내는 것입니다.  \n",
      "- 단, 확률적 선택이므로 틀릴 수도 있고, 최신 정보나 개인 사생활은 기억하지 못합니다.\n",
      "\n",
      "--------------------------------------------------\n",
      "핵심 한 줄 요약\n",
      "--------------------------------------------------\n",
      "ChatGPT는 “거대한 Transformer 기반 자기 회귀 언어 모델”이며,  \n",
      "“웹 전체를 학습한 뒤 사람이 선호하는 대화 품질을 강화학습으로 끌어올린”  \n",
      "확률적 문자 생성기라고 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. \\\n",
    "     Please provide clear and detailed explanations.\"),\n",
    "    (\"human\", \"{model_name} 모델의 학습 원리를 설명해 주세요.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# 생성한 메시지를 바로 주입하여 호출하기\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "# 출력은 구조화된 AIMessage 타입이며, 내용을 쓰려면 response.content로 접근\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ChatGPT(또는 GPT 계열 모델)가 “왜 질문에 대해 그런 답변을 할 수 있는가?”는 단 한 가지 학습 목표에서 시작됩니다.  \n",
      "“주어진 앞쪽 토큰(token)들을 보고 다음 토큰을 맞춰라.”  \n",
      "이 단순한 과제를 대규모 신경망, 대규모 데이터, 대규모 컴퓨팅으로 반복하면 ‘대화 능력’이 자연스럽게 생겨납니다. 아래에 1) 학습 단계별로, 2) 핵심 원리별로 나누어 설명하겠습니다.\n",
      "\n",
      "----------------------------------------\n",
      "1. 학습 단계별 흐름\n",
      "----------------------------------------\n",
      "1. Pre-training (사전 학습)  \n",
      "   - 데이터 : 공공 웹, 위키피디아, 책, GitHub 등 수천억 토큰  \n",
      "   - 목표 : 다음 토큰 예측 (자기 회귀 언어 모델링)  \n",
      "   - 결과물 : 기본(Base) LM → ‘세계 지식’은 있지만 ‘사용자의 뜻’을 잘 따르지 못함.\n",
      "\n",
      "2. SFT (Supervised Fine-Tuning, 지도 미세 조정)  \n",
      "   - 데이터 : 사람이 작성한 (프롬프트, 이상적인 답변) 쌍 1~10만 개  \n",
      "   - 목표 : “좋은 답변이라고 판단되는 텍스트”를 모방하도록 추가 학습  \n",
      "   - 결과물 : 지시-응답(Instruction-Following) 모델.\n",
      "\n",
      "3. Reward Modeling (보상 모델 학습)  \n",
      "   - 데이터 : 같은 프롬프트에 대한 여러 답변을 사람이 1~5 점 또는 순위 매김  \n",
      "   - 목표 : 사람의 선호를 예측하는 RM(보상 모델)을 만듦.\n",
      "\n",
      "4. RLHF (Reinforcement Learning from Human Feedback)  \n",
      "   - 알고리즘 : PPO (Proximal Policy Optimization)  \n",
      "   - 목표 : RM이 주는 보상 점수를 높이도록 SFT 모델의 파라미터를 미세 조정.  \n",
      "   - 결과물 : 사람이 더 선호하는 답변을 생성하는 정책(policy) 획득.\n",
      "\n",
      "※ GPT-4 이후에는 보안·정직성·거절 능력을 추가로 강화하는 단계(PPO+extra data, Rule-based reward, red-teaming 등)가 더 붙지만, 핵심 흐름은 위와 같습니다.\n",
      "\n",
      "----------------------------------------\n",
      "2. 핵심 원리 5가지\n",
      "----------------------------------------\n",
      "1. Transformer 아키텍처  \n",
      "   - Self-attention : 문장 전체 토큰 간 관계를 병렬로 계산 → 긴 맥락 파악 가능  \n",
      "   - Positional encoding : 단순 ‘단어 가방’이 아니라 순서 정보를 유지  \n",
      "   - Decoder-Only 구조 : GPT는 왼쪽→오른쪽만 보고 다음 토큰을 예측\n",
      "\n",
      "2. 확률적 다음 토큰 예측  \n",
      "   - P(wₜ | w₁…wₜ₋₁; θ)를 Neural Net으로 근사.  \n",
      "   - 훈련 시 Cross-Entropy 손실 최소화.  \n",
      "   - 생성 시 Temperature, Top-p 등 샘플링 전략으로 다양성·정확도 조절.\n",
      "\n",
      "3. 분산 표현(Embedding)  \n",
      "   - 각 토큰은 d차원(예: 12288) 벡터.  \n",
      "   - Attention 레이어는 이 벡터들을 ‘의미’ 공간에서 회전·결합 → 문법·의미·사실 정보를 동시에 압축.\n",
      "\n",
      "4. 확장 법칙(Scaling Laws)  \n",
      "   - “모델 크기↑ × 데이터 양↑ × 컴퓨트↑ → 테스트 손실↓”  \n",
      "   - 지수적 증가로도 성능이 선형적으로 향상(일정 범위 내).  \n",
      "   - GPT-3→GPT-4로 갈수록 파라미터 수·학습 토큰 수·GPU 수 모두 큰 폭 증가.\n",
      "\n",
      "5. 정렬(Alignment) 기술  \n",
      "   - RLHF로 ‘사람이 선호하는 스타일’을 학습 → 유해·차별 응답 줄이고, 거절 능력 향상.  \n",
      "   - KL-정규화 항을 추가해 원본 모델과 너무 멀리 가지 않도록 제약.  \n",
      "   - 여전히 완전한 정렬은 어렵기 때문에, 시스템 프롬프트·필터링·모니터링이 병행됨.\n",
      "\n",
      "----------------------------------------\n",
      "3. 전체 흐름 요약\n",
      "----------------------------------------\n",
      "1. 대규모 텍스트에서 “다음 단어 맞추기”만 반복 → 세계 지식과 언어 확률 모델 획득  \n",
      "2. 사람의 명령-응답 예시로 추가 학습 → 지시 따르기 능력 향상  \n",
      "3. 사람의 순위·점수를 보상으로 삼아 강화 학습 → 유용·안전·친화적 답변 생성\n",
      "\n",
      "결국 ChatGPT는 “확률적 다음 토큰 생성기”에 불과하지만, 이를 거대화·미세조정·정렬하면 ‘대화형 지능’이 우리가 체감할 수준으로 나타나는 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 체인을 생성하여 호출하기\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplate와 HumanMessagePromptTemplate 클래스 사용\n",
    "* 객체 지향적 접근 - Message 객체를 독립적으로 생성 가능\n",
    "* 여러 조건에 따라 다른 시스템 메시지 선택\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"초보자를 위한 설명: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"전문가를 위한 상세 분석: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ChatMessagePromptTemplate 활용\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM 호출\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplate는 여러 종류의 메시지(시스템, 인간, AI)를 조합하여 복잡한 프롬프트를 생성할 때 유용합니다.\n",
    "* SystemMessagePromptTemplate: 이 템플릿은 AI 모델에게 역할을 부여하거나 전반적인 규칙을 설정하는 시스템 메시지를 만듭니다. 위의 예시에서는 \"번역을 도와주는 유용한 도우미\"라는 역할을 지정합니다.\n",
    "* HumanMessagePromptTemplate: 이 템플릿은 사용자의 질문이나 요청을 담는 인간 메시지를 만듭니다. 아래의 예시에서는 번역할 텍스트를 입력받습니다.\n",
    "* ChatPromptTemplate.from_messages: 이 클래스 메서드는 시스템 메시지, 인간 메시지 등 여러 종류의 MessagePromptTemplate 객체들을 리스트로 받아 하나의 채팅 프롬프트 템플릿으로 통합합니다.\n",
    "* format_messages: 이 메서드는 정의된 템플릿에 실제 값을 채워 넣어 [SystemMessage, HumanMessage] 형태의 리스트를 반환합니다. 이 리스트는 채팅 모델(Chat Model) 에 바로 전달될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "나는 프로그래밍을 사랑해.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplate와 HumanMessagePromptTemplate 생성\n",
    "# SystemMessagePromptTemplate는 모델의 페르소나 또는 기본 지침을 설정합니다.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplate는 사용자로부터 받는 입력 프롬프트를 정의합니다.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate 생성\n",
    "# 위에서 만든 두 템플릿을 리스트로 묶어 ChatPromptTemplate을 만듭니다.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. 프롬프트 포맷팅\n",
    "# chat_prompt_template.format_messages()를 사용하여 최종 메시지 리스트를 생성합니다.\n",
    "# 이 함수는 딕셔너리 형태의 입력 변수를 받습니다.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM 호출\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplate은 모델이 특정 형식을 따르게 하거나, 일관된 응답을 생성하도록 유도할 때 유용합니다.\n",
    "* 도메인 지식이 필요하거나, AI가 오답을 줄이고 더 신뢰할 만한 답변을 생성하도록 해야 할 때 효과적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplate을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "태양계 행성(수성→목성까지)을 “거리-크기-특징” 한 줄로 압축하면 다음과 같습니다.\n",
      "\n",
      "1. **수성** : 태양에서 가장 가까고 작으며, 표면이 달처럼 운석坑 가득하다.  \n",
      "2. **금성** : 지구와 유사한 크기이나 온실효과로 470 °C 돌아 구리는 ‘지옥행성’이다.  \n",
      "3. **지구** : 유일한 액체 물이 흐르고 생명이 존재하는 행성이다.  \n",
      "4. **화성** : 붉은 철鏽 덮인 사막행성으로, 얼음이 있고 인류 탐사의 다음 목표다.  \n",
      "5. **목성** : 태양계 최대 기체행성으로, 대적점·목성고리·79개 위성(가니메데 등)을 품었다.  \n",
      "6. **토성** : 아름다운 고리와 82개 위성(타이탄·농밀 대기)을 지닌 기체거인이다.  \n",
      "7. **천왕성** : 옆으로 누워 자전하며, 淡青色의 얼음행성으로 고리와 27개 위성이 있다.  \n",
      "8. **해왕성** : 태양계 최강 바람(초속 600 m)이 부는 푸른 얼음행성으로 14개 위성(트리톤 등)을 거느렸다.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate을 사용하지 않는 경우\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain 실행\n",
    "result = llm.invoke(\"태양계의 행성들을 간략히 정리해 주세요.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001B13BB5A180> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B13C0935C0> root_client=<openai.OpenAI object at 0x000001B139080BC0> root_async_client=<openai.AsyncOpenAI object at 0x000001B13BBCC740> model_name='moonshotai/kimi-k2-instruct-0905' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 초등학생도 쉽게 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': '뉴턴의 운동 법칙을 요약해 주세요.', 'output': '### 뉴턴의 운동 법칙\\n1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\\n2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\\n3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.'}, {'input': '지구의 대기 구성 요소를 알려주세요.', 'output': '### 지구 대기의 구성\\n- **질소 (78%)**: 대기의 대부분을 차지합니다.\\n- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\\n- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\\n- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B13BB5A180>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B13C0935C0>, root_client=<openai.OpenAI object at 0x000001B139080BC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B13BBCC740>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "양자컴퓨터는 ‘코인’이 아니라 ‘동전’으로 장난치는 초특급 장난감이에요!\n",
      "\n",
      "1. 일반 컴퓨터\n",
      "- 동전을 던져서 앞·뒤(0 또는 1)만 기록\n",
      "- 한 번에 한 가지 상태만 볼 수 있음\n",
      "\n",
      "2. 양자 컴퓨터\n",
      "- 동전을 ‘세계에서 제일 빠르게’ 돌려서\n",
      "- 앞과 뒤가 ‘동시에 섞인’ 상태(양자 상태)를 만들어요\n",
      "- 이렇게 하면 한꺼번에 여러 계산을 ‘겹쳐서’ 할 수 있어서\n",
      "- 특정 퍼즐(예: 큰 숫자를 곱하거나, 비밀번호 찾기)을 훨씬 빨리 풀 수 있어요\n",
      "\n",
      "정리\n",
      "- 동전을 ‘겹치게’ 돌리는 마술 → 양자 동전\n",
      "- 이 마술로 ‘여러 답을 동시에’ 찾아내는 컴퓨터 → 양자 컴퓨터\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate 사용하는 경우\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 쉽게 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(llm)\n",
    "print(chain)\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"양자컴퓨팅에 대하여 설명해 주세요.\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* 프롬프트를 더 동적으로 활용할 수 있으며, AI 응답을 더 일관성 있게 조정 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 프롬프트: 가을에 일어나는 대표적인 지구과학 현상은 태풍 발생이 맞나요?         가을에 주로 발생하는 지구과학 현상을 3개 알려주세요\n",
      " 모델 응답: 가을에 주로 발생하는 **지구과학 현상** 3가지를 정확히 말씀드리면:\n",
      "\n",
      "1. **대기냉각(大氣冷卻)**: 여름 동안 가열된 육지가 빠르게 식으면서 기압 차가 커지고, **가을철 큰 일교차**와 **고기압의 발달**이 뚜렷해집니다.  \n",
      "2. **가을철 해일(秋の高潮)**: 태풍이나 저기압의 영향으로 **가을철 해일**이 자주 발생합니다. 특히 9~10월에 태풍이 북상하면서 **스톰서지(storm surge)**가 유발됩니다.  \n",
      "3. **황사(黃砂)의 재분출**: 중국 내몽골 지역에서 **가을철 건조한 기류**로 인해 먼지가 일어나 **황사**가 발생하기도 합니다. 봄철보다는 빈도가 낮지만, **가을 황사**도 관측됩니다.\n",
      "\n",
      "---\n",
      "\n",
      "**태풍 발생**은 **여름~가을(7~9월)**에 걸쳐서 활발하지만, **가을에만 특정되는 현상은 아닙니다.**  \n",
      "따라서 **“가을에 일어나는 대표적인 지구과학 현상 = 태풍 발생”**이라고 말하기보다는,  \n",
      "**“가을철 해일”**이나 **“대기냉각”** 같은 계절적 특성이 더 정확합니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# 프롬프트 템플릿 정의 (부분 변수 적용)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}이 맞나요? \\\n",
    "        {season}에 주로 발생하는 지구과학 현상을 3개 알려주세요\",\n",
    "    input_variables=[\"phenomenon\"],  # 사용자 입력 필요\n",
    "    partial_variables={\"season\": get_current_season()}  # 동적으로 계절 값 할당\n",
    ")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# 특정 계절의 현상 질의\n",
    "query = prompt.format(phenomenon=\"태풍 발생\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\" 프롬프트: {query}\")\n",
    "print(f\" 모델 응답: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 계절: 가을\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# Step 1: 현재 계절 결정\n",
    "season_name = get_current_season()  # 계절 값 얻기\n",
    "print(f\"현재 계절: {season_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 가을에 발생하는 자연 현상:\n",
      "가을철에 뚜렷하게 나타나는 대표적인 지구과학 현상 3가지는 다음과 같습니다.\n",
      "\n",
      "1. 시베리아 고기압 확장  \n",
      "   아시아 대륙이 급속히 식으면서 시베리아 상공에 강한 고기압이 형성되고, 이 고기압이 한반도까지 남하하면서 맑고 건조한 가을 날씨를 만듭니다. 이 현상이 주로 9~10월에 뚜렷해집니다.\n",
      "\n",
      "2. 노을·단풍 현상  \n",
      "   고위도에서 내려온 건조한 공기가 맑은 날씨를 유지하면서 해질녘 대기 중 먼지·연기 등 에어로졸이 증가해 붉은 노을이 잘 발생합니다. 동시에 기온이 낮아지면서 단풍나무 등에서 안토시아닌 색소가 합성되어 단풍이 절정을 이룹니다.\n",
      "\n",
      "3. 계절풍(동북계절풍) 정착  \n",
      "   대륙과 해양 사이의 온도 차가 커지면서 북서~북동에서 불어오는 차고 건조한 계절풍이 안정적으로 한반도에 정착합니다. 이 바람은 가을철 낮은 습도와 강한 일교차를 만들어 주요 기후 특징으로 작용합니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 해당 계절의 자연 현상 추천\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}에 주로 발생하는 대표적인 지구과학 현상 3가지를 알려주세요. \"\n",
    "    \"각 현상에 대해 간단한 설명을 포함해주세요.\"\n",
    ")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatOpenAI(\n",
    "#     #api_key=OPENAI_API_KEY,\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# 체인 2: 자연 현상 추천 (입력: 계절 → 출력: 자연 현상 목록)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season_name}  # chain1의 출력을 season 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: 현재 계절에 따른 자연 현상 추천\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season_name}에 발생하는 자연 현상:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API 호출 데이터, 시간 정보, 사용자 정보 등을 반영할 때 매우 유용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 프롬프트: 현재 1달러 = 1401.17원 기준으로 환율 정보를 알려드립니다. 이에 대한 분석을 제공해 주세요.\n",
      " 모델 응답: 환율: 1달러 = 1,401.17원  \n",
      "(기준일: 2024-06-XX, 뱅크오브코리아 기준 종가 기준)\n",
      "\n",
      "1. 수치적 의미\n",
      "- ‘1400원 돌파’는 2022년 11월 이후 1년 7개월 만의 사상 두 번째 고점(최고치 1,452.10원/2022-10-11).\n",
      "- 연초(1,260원 대) 대비 약 11% 절상, 3개월 전(4월 초 1,320원) 대비에도 6% 이상 급등.\n",
      "- 2022년 고점 대비 아직 3.5% 남은 ‘여유’가 있지만, 1,400원은 심리적 저항선이자 수입·수출 기업의 손익 분기점이 되는 키 레벼.\n",
      "\n",
      "2. 급등의 3가지 핵심 동인\n",
      "(1) 달러 강세(DXY 105→107↑)  \n",
      "- 미 연준의 ‘higher-for-longer’ 어조 재확인(6월 FOMC 닷플롯 3.75→4.0% 상향)  \n",
      "- 유럽 ECB 선제 인하 기대 vs. FRB 동결 시 차익거래(캐리) 유입  \n",
      "→ 달러 인덱스 상승 → 원화 약세\n",
      "\n",
      "(2) 한국 내 외화 유동성 긴장  \n",
      "- 5월 이후 수출 호조(반도체 50%↑, 자동차 9%↑)에도 불구,  \n",
      "  · 중동·러시아 프로젝트 건설 자재 선적로 인한 실수요 증가  \n",
      "  · 해외 증시 사상 최고가에 따른 개인 해외주식 순매수(1~5월 누적 120억달러)  \n",
      "→ 은행 외화대출·한국증권예탁원 예택잔 고갈 → 스왑포인트 급등(1년 300bp→420bp)\n",
      "\n",
      "(3) 중국 경기 둔화 & 위안화 약세(USD/CNY 7.10→7.25)  \n",
      "- 한국 수출의 25%가 중국행 → 위안화 약세는 수콘(수출 경쟁력) 저하 우려 → 외국인 자금 이탈 → 원화 약세 악순환\n",
      "\n",
      "3. 거시·금융시장 영향\n",
      "- 물가: 수입물가지수(달러표시) 1% 올라도 CPI 0.04%p 상승. 1400원 지속 시 연간 CPI +0.3%p 우려.\n",
      "- 기업 실적: 1,350원→1,400원(3.7%↑), 수입원가 1% 상승 → 제조업 영업이익 –0.8%p 하락.\n",
      "- 증시: 외국인 6월 누적 4조원 순매도(코스피 –3.2%). IT·화학·항공 등 달러 수입 비중 높은 섹터 조정.\n",
      "- 채권: 3년 국채 3.60%→3.80%(금리 상승). 10년물 4.20% 돌파. 뱅크오브코리아 7월 기준금리 동결 확률 60%→40%로 하락.\n",
      "\n",
      "4. 한국은행·정책당국 대응\n",
      "- 외환당국 “시장 혼란 시 긴급 안정 조치”(6·20 담화) → 내부적 ‘버퍼’ 50억달러 규모 실효구간 진입 가능성.\n",
      "- FX 스왑 계약(한·미 600억달러, 한·중 400억달러) 가동은 아직까지 미동원. 단, 1,450원 돌파 시 USDKRW 스왑라인 급등 → 물가·금리 부담 가중.\n",
      "- 금융통화위원회: 7월 0.25%p 인하(총통) 가능성 25%→15%로 축소. CPI 2% 달성이 2025년 2분기→3분기로 연기.\n",
      "\n",
      "5. 향후 전망(베이스·베어·불 시나리오)\n",
      "- 베이스(가능성 55%): 2H 미국 경기 착륙·연준 9월 첫 인하, 유럽·중국 경기 반등 → 3분기 1,360~1,390원, 연말 1,300~1,330원.\n",
      "- 베어(30%): 중동 전격전·유가 100불, 미 인플레 재점화 → 연준 12월 금리 동결 유지 → 3분기 1,450~1,480원.\n",
      "- 불(15%): 미 경기 급격한 경착륙(실업률 5%↑) → 연준 선제 50bp 인하 → 안전자산 선호 완화 → 4분기 1,250원 아래.\n",
      "\n",
      "6. 민감 그룹별 체크포인트\n",
      "- 수입기업: 1,400원 시점에서 선물환 매도(6개월물 1,385원) 락인(lock-in). 원자재 재고 회전율 2개월 미만은 헤지 비율 70% 이상 권장.\n",
      "- 수출기업: 달러표시 가격 경쟁력 악화. KRW약세 수혜는 화장품·중소형선박·게임. 반도체는 가격 반등이 환율 상쇄.\n",
      "- 개인투자자: 해외주식 투자 시 환율헤지 ETF(KODEX USD Hedge 등) 비중 30~50% 확대. 달러보다는 엔·위안 표현 자산 단기 회피.\n",
      "- 가계: 해외 직구·여행 비용 10% 이상 증가. 해외결제 카드는 환율 우대 0.5~1.0% 차이 발생. 여행 목표 통화(USD) 50%는 선물환으로 사전 확보.\n",
      "\n",
      "7. 결론\n",
      "“1,400원은 단순 심리 저항선이 아니라 한국 경제의 ‘신호등’”\n",
      "- 1,350원 이하로 안착하지 않는 한 기준금리 인하 공간은 제한.\n",
      "- 수출 회복세가 지속되더라도 외화 수급 불균형이 맞물리면 1,450원 고점 재도전 가능성이 3분기 내 30%를 웃돈다.\n",
      "- 정책당국은 ‘시장 개입’보다는 외화 유동성 공급(스왑라인, 외화대출 한도 확대)로 변동성을 완화하는 쪽으로 방향을 잡고 있다.\n",
      "- 기업과 가계는 ‘환율 10% 변동에 대한 손익 시나리오’를 반드시 작성하고, 헤지·소비·투자 타이밍을 조정해야 한다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 실시간 환율을 가져오는 함수\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1달러 = {data['rates']['KRW']}원\"\n",
    "\n",
    "# Partial Prompt 활용\n",
    "prompt = PromptTemplate(\n",
    "    template=\"현재 {info} 기준으로 환율 정보를 알려드립니다. 이에 대한 분석을 제공해 주세요.\",\n",
    "    input_variables=[],  # 사용자 입력 없음\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # API에서 가져온 데이터 자동 반영\n",
    ")\n",
    "\n",
    "# LLM 모델 설정\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# 모델에 프롬프트 전달 및 응답 받기\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# 결과 출력\n",
    "print(\" 프롬프트:\", prompt.format())\n",
    "print(\" 모델 응답:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-auRXAaAq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
