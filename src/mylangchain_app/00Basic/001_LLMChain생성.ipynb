{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a",
   "metadata": {
    "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a"
   },
   "source": [
    "### LLM Chain 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9",
   "metadata": {
    "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9"
   },
   "source": [
    "## 1. 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81",
   "metadata": {
    "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81"
   },
   "source": [
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
    "outputId": "c96ed02d-19b7-4e90-d92e-1ae52895e303"
   },
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI 인증키 설정\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_4\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09aaca6-5aa2-4b52-bbfc-196e808dc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.27\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c",
   "metadata": {
    "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c"
   },
   "source": [
    "## 2. LLM Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23729d10-9600-415b-b7d1-f954665224e3",
   "metadata": {
    "id": "23729d10-9600-415b-b7d1-f954665224e3"
   },
   "source": [
    "### 1) Prompt + LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "on0y4xF8VoyE",
   "metadata": {
    "id": "on0y4xF8VoyE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 실행\n",
    "result = llm.invoke(\"인공지능 모델의 학습 원리에 대하여 쉽게 설명해 주세요.\")\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc4accf-c927-40a3-ba2c-f891c94c34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='인공신경망은 사람의 뇌처럼 수많은 뉴런이 서로 연결된 형태를 가지고 있습니다. 각 뉴런은 입력된 정보를 처리하여 다른 뉴런으로 전달하는 역할을 합니다.\\n\\n인공신경망은 입력층, 은닉층, 출력층으로 구성되어 있습니다. 입력층은 데이터를 입력받는 역할을 하고, 은닉층은 데이터를 처리하는 역할을 하며, 출력층은 결과를 출력하는 역할을 합니다.\\n\\n인공신경망의 학습 원리는 다음과 같습니다.\\n\\n1.  **데이터 수집**: 인공신경망은 학습을 위해 데이터를 수집합니다. 이 데이터는 입력 데이터와 해당 입력 데이터에 대한 올바른 출력 데이터로 구성됩니다.\\n2.  **모델 초기화**: 인공신경망은 가중치와 편향이라는 매개변수를 초기화합니다. 가중치는 뉴런 간의 연결 강도를 나타내고, 편향은 뉴런의 출력에 영향을 미치는 상수입니다.\\n3.  **순전파**: 인공신경망은 입력 데이터를 입력층에 입력하고, 각 뉴런에서 데이터를 처리하여 출력층으로 전달합니다. 이 과정에서 가중치와 편향이 사용됩니다.\\n4.  **오차 계산**: 인공신경망은 출력층에서 출력된 결과와 실제 출력 데이터 간의 오차를 계산합니다.\\n5.  **역전파**: 인공신경망은 오차를 역으로 전파하여 각 뉴런의 가중치와 편향을 업데이트합니다. 이 과정에서 경사 하강법이라는 최적화 알고리즘이 사용됩니다.\\n6.  **반복**: 인공신경망은 3~5 단계를 반복하여 학습합니다. 반복 횟수는 학습 데이터의 양, 모델의 복잡도 등에 따라 결정됩니다.\\n\\n인공신경망의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 사람의 뇌는 뉴런 간의 연결 강도를 조절하여 학습하고, 인공신경망도 가중치를 조절하여 학습합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 404, 'prompt_tokens': 24, 'total_tokens': 428, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.200446222, 'prompt_time': 0.000667306, 'completion_time': 1.009017649, 'total_time': 1.009684955}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_5d3e4e58e1', 'id': 'chatcmpl-eca60236-63c4-48c6-a65e-6c4754c82b8c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--88ad0fc0-a3f3-475d-a4a2-f99a9864826a-0' usage_metadata={'input_tokens': 24, 'output_tokens': 404, 'total_tokens': 428, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "WzcZy4PruV1n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "WzcZy4PruV1n",
    "outputId": "18ecc8f9-5748-4b16-cb07-4a0f7a01fb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공신경망은 사람의 뇌처럼 수많은 뉴런이 서로 연결된 형태를 가지고 있습니다. 각 뉴런은 입력된 정보를 처리하여 다른 뉴런으로 전달하는 역할을 합니다.\n",
      "\n",
      "인공신경망은 입력층, 은닉층, 출력층으로 구성되어 있습니다. 입력층은 데이터를 입력받는 역할을 하고, 은닉층은 데이터를 처리하는 역할을 하며, 출력층은 결과를 출력하는 역할을 합니다.\n",
      "\n",
      "인공신경망의 학습 원리는 다음과 같습니다.\n",
      "\n",
      "1.  **데이터 수집**: 인공신경망은 학습을 위해 데이터를 수집합니다. 이 데이터는 입력 데이터와 해당 입력 데이터에 대한 올바른 출력 데이터로 구성됩니다.\n",
      "2.  **모델 초기화**: 인공신경망은 가중치와 편향이라는 매개변수를 초기화합니다. 가중치는 뉴런 간의 연결 강도를 나타내고, 편향은 뉴런의 출력에 영향을 미치는 상수입니다.\n",
      "3.  **순전파**: 인공신경망은 입력 데이터를 입력층에 입력하고, 각 뉴런에서 데이터를 처리하여 출력층으로 전달합니다. 이 과정에서 가중치와 편향이 사용됩니다.\n",
      "4.  **오차 계산**: 인공신경망은 출력층에서 출력된 결과와 실제 출력 데이터 간의 오차를 계산합니다.\n",
      "5.  **역전파**: 인공신경망은 오차를 역으로 전파하여 각 뉴런의 가중치와 편향을 업데이트합니다. 이 과정에서 경사 하강법이라는 최적화 알고리즘이 사용됩니다.\n",
      "6.  **반복**: 인공신경망은 3~5 단계를 반복하여 학습합니다. 반복 횟수는 학습 데이터의 양, 모델의 복잡도 등에 따라 결정됩니다.\n",
      "\n",
      "인공신경망의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 사람의 뇌는 뉴런 간의 연결 강도를 조절하여 학습하고, 인공신경망도 가중치를 조절하여 학습합니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c97fc",
   "metadata": {},
   "source": [
    "### 2) PromptTemplate + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "SeNi_VXqYD-b",
   "metadata": {
    "id": "SeNi_VXqYD-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01WLucSpYjZt",
   "metadata": {
    "id": "01WLucSpYjZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \\n\\n기본적으로 인공지능 모델은 데이터를 통해 학습합니다. 예를 들어, 사진을 고양이와 강아지로 구분하는 모델을 만든다고 가정해봅시다. \\n\\n1. **데이터 수집**: 수많은 고양이와 강아지의 사진 데이터를 수집합니다. 각 사진에는 '고양이' 또는 '강아지'라는 라벨이 붙어 있습니다.\\n\\n2. **모델 초기화**: 처음에는 모델이 고양이와 강아지의 특징을 전혀 모릅니다. 모델은 단순히 입력된 사진의 픽셀 값들을 보고 출력을 생성합니다.\\n\\n3. **예측**: 모델에 새로운 사진을 입력하면, 모델은 이 사진이 고양이인지 강아지인지를 예측합니다. 처음에는 정확하지 않을 것입니다.\\n\\n4. **오차 계산**: 모델의 예측과 실제 라벨(고양이 또는 강아지)을 비교하여 오차를 계산합니다. 이 오차를 '손실(loss)'이라고 합니다.\\n\\n5. **모델 업데이트**: 모델은 이 오차를 줄이기 위해 자신의 내부 파라미터들(가중치와 편향)을 조금씩 조정합니다. 이 과정은 '역전파(backpropagation)'라고 하며, 모델이 올바른 방향으로 나아가기 위해 필요한 과정입니다.\\n\\n6. **반복**: 3~5번의 과정을 매우 많은 사진 데이터에 대해 반복합니다. 매번 모델은 조금씩 더 좋아져서, 고양이와 강아지를 더 정확하게 구분하게 됩니다.\\n\\n7. **수렴**: 결국 모델은 대부분의 고양이와 강아지 사진을 정확하게 구분할 수 있게 됩니다. 이 때 모델은 학습이 완료되었다고 말합니다.\\n\\n이 과정을 통해 인공지능 모델은 주어진 데이터를 통해 스스로 규칙을 발견하고, 미래의 새로운 데이터에 대해 예측할 수 있는 능력을 습득하게 됩니다.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 36, 'total_tokens': 414, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.204238141, 'prompt_time': 0.000959961, 'completion_time': 0.903926964, 'total_time': 0.904886925}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_5d3e4e58e1', 'id': 'chatcmpl-60bb62e7-71c6-499e-981c-42b5efea6af1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--3e2933a1-e7c6-4547-a1cb-180d8e1bea4f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 378, 'total_tokens': 414, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))\n",
    "\n",
    "# chain 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170ec878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \n",
      "\n",
      "기본적으로 인공지능 모델은 데이터를 통해 학습합니다. 예를 들어, 사진을 고양이와 강아지로 구분하는 모델을 만든다고 가정해봅시다. \n",
      "\n",
      "1. **데이터 수집**: 수많은 고양이와 강아지의 사진 데이터를 수집합니다. 각 사진에는 '고양이' 또는 '강아지'라는 라벨이 붙어 있습니다.\n",
      "\n",
      "2. **모델 초기화**: 처음에는 모델이 고양이와 강아지의 특징을 전혀 모릅니다. 모델은 단순히 입력된 사진의 픽셀 값들을 보고 출력을 생성합니다.\n",
      "\n",
      "3. **예측**: 모델에 새로운 사진을 입력하면, 모델은 이 사진이 고양이인지 강아지인지를 예측합니다. 처음에는 정확하지 않을 것입니다.\n",
      "\n",
      "4. **오차 계산**: 모델의 예측과 실제 라벨(고양이 또는 강아지)을 비교하여 오차를 계산합니다. 이 오차를 '손실(loss)'이라고 합니다.\n",
      "\n",
      "5. **모델 업데이트**: 모델은 이 오차를 줄이기 위해 자신의 내부 파라미터들(가중치와 편향)을 조금씩 조정합니다. 이 과정은 '역전파(backpropagation)'라고 하며, 모델이 올바른 방향으로 나아가기 위해 필요한 과정입니다.\n",
      "\n",
      "6. **반복**: 3~5번의 과정을 매우 많은 사진 데이터에 대해 반복합니다. 매번 모델은 조금씩 더 좋아져서, 고양이와 강아지를 더 정확하게 구분하게 됩니다.\n",
      "\n",
      "7. **수렴**: 결국 모델은 대부분의 고양이와 강아지 사진을 정확하게 구분할 수 있게 됩니다. 이 때 모델은 학습이 완료되었다고 말합니다.\n",
      "\n",
      "이 과정을 통해 인공지능 모델은 주어진 데이터를 통해 스스로 규칙을 발견하고, 미래의 새로운 데이터에 대해 예측할 수 있는 능력을 습득하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56848a4c",
   "metadata": {},
   "source": [
    "### 3) PromptTemplate + LLM(invoke()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1e9009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'str'>\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 컴퓨터에大量的 데이터를 제공하고, 데이터를 분석하고 패턴을 학습하도록 합니다.\n",
      "\n",
      "예를 들어, 고양이와 개 사진을 구분하는 모델을 만든다고 가정해 봅시다.\n",
      "\n",
      "1. **데이터 수집**: 많은 고양이와 개 사진 데이터를 수집합니다.\n",
      "2. **데이터 전처리**: 사진을 픽셀 단위로 나누어 숫자로 변환합니다. (예: 224 x 224 픽셀 이미지 -> 224 x 224 행렬)\n",
      "3. **모델 초기화**: 신경망 모델을 생성하고 가중치를 무작위로 초기화합니다.\n",
      "4. **학습**: 고양이/개 사진 데이터를 모델에 입력하고, 모델은 예측 결과를 출력합니다. (예: 이 사진은 고양이 80%, 개 20%)\n",
      "5. **손실 함수 계산**: 예측 결과와 실제 레이블(고양이/개) 간의 오류를 계산합니다. (예: 고양이 사진인데 80% 고양이, 20% 개로 예측한 경우 오류가 크므로 손실 함수 값이 큼)\n",
      "6. **역전파**: 손실 함수 값을 최소화하기 위해, 각 신경망 층의 가중치를 업데이트합니다. (예: 가중치를 조금 더 고양이 특징에 민감하게 조정)\n",
      "7. **반복**: 4-6 단계를 반복하며 모델의 성능을 향상시킵니다.\n",
      "\n",
      "이 과정을 통해 모델은 고양이와 개의 특징을 스스로 학습하고, 새로운 사진을 입력했을 때 고양이/개를 구분할 수 있는 능력을 갖추게 됩니다.\n",
      "\n",
      "즉, 인공지능 모델의 학습 원리는 다음과 같습니다.\n",
      "\n",
      "* **데이터**: 학습에 필요한大量的 데이터\n",
      "* **예측**: 모델의 예측 결과\n",
      "* **오류**: 예측 결과와 실제 레이블 간의 오류\n",
      "* **업데이트**: 모델의 가중치를 업데이트하여 오류를 최소화\n",
      "\n",
      "이러한 학습 원리를 통해 인공지능 모델은 다양한 작업에 활용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. \\\n",
    "                                      Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. chain 생성 (LCEL)\n",
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "\n",
    "# 3. chain의 invoke 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d997b16",
   "metadata": {},
   "source": [
    "### 4) PromptTemplate + LLM(stream()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "684654e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \n",
      "\n",
      "기본적으로 인공지능 모델은 데이터를 통해 학습합니다. 예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 만든다고 가정해 봅시다. 이 모델에 고양이와 강아지의 사진을 여러 장 보여주고, 이 사진들이 고양이인지 강아지인지를 알려주면 모델은 이 데이터를 통해 학습합니다.\n",
      "\n",
      "학습 과정은 다음과 같습니다:\n",
      "\n",
      "1. **데이터 수집**: 다양한 고양이와 강아지의 사진을 수집합니다.\n",
      "2. **데이터 라벨링**: 각 사진에 고양이 또는 강아지라는 라벨을 붙입니다.\n",
      "3. **모델 훈련**: 수집한 데이터를 모델에 입력하고, 모델이 예측한 결과와 실제 라벨을 비교합니다.\n",
      "4. **오차 계산**: 모델의 예측과 실제 라벨 사이의 오차를 계산합니다.\n",
      "5. **모델 업데이트**: 오차를 줄이기 위해 모델의 내부 파라미터를 조정합니다.\n",
      "6. **반복**: 3~5번 과정을 여러 번 반복합니다.\n",
      "\n",
      "이 과정을 통해 모델은 고양이와 강아지의 특징을 스스로 학습하고, 새로운 사진을 입력받았을 때 고양이인지 강아지인지 높은 확률로 분류할 수 있게 됩니다.\n",
      "\n",
      "이러한 학습 원리는 신경망을 기반으로 하는 딥러닝 모델에서 특히 중요합니다. 신경망은 여러 층의 노드로 구성되어 있으며, 각 노드는 입력 데이터를 처리하고 출력값을 생성합니다. 학습 과정에서 각 노드의 가중치가 조정되어 모델의 성능이 개선됩니다.\n",
      "\n",
      "예를 들어, 사진을 입력받았을 때 모델은 다음과 같은 과정을 거칩니다:\n",
      "\n",
      "* **입력층**: 사진 데이터를 입력받습니다.\n",
      "* **은닉층**: 여러 층의 노드에서 사진의 특징을 추출합니다 (예: 고양이의 눈, 코, 귀 등).\n",
      "* **출력층**: 최종적으로 고양이 또는 강아지라는 결과를 출력합니다.\n",
      "\n",
      "모델의 성능은 학습 데이터의 양과 질, 모델의 구조, 학습 알고리즘 등에 따라 달라집니다. 우수한 성능을 내는 모델을 만들기 위해서는 데이터의 수집과 라벨링, 모델의 설계, 학습 과정의 최적화 등이 중요합니다."
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. \\\n",
    "                                      Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "lm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "# 스트리밍 출력\n",
    "#print(answer)\n",
    "\n",
    "for token in answer:\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0188674-915f-41af-ac46-56f9f54289b0",
   "metadata": {
    "id": "b0188674-915f-41af-ac46-56f9f54289b0"
   },
   "source": [
    "##### 2) Multiple Chains\n",
    "* Multi Chain을 활용한 영화 추천 및 줄거리 요약 ( 잘 동작하지 않는 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
    "outputId": "7ee83878-5d1a-45e3-f033-100da33f3ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " ===> 추천된 영화:\n",
      "드라마 장르라면 **『셰익스피어 인 러브』(1998)**를 꼽고 싶습니다.  \n",
      "사랑과 창작의 고통, 시대적 제약이 부딪히는 순간을 코믹하면서도 애절하게 풀어낸 작품이에요. 런던의 무대 위에서 꿈을 키우는 극작가 ‘윌’과 극단의 귀공녀 ‘비올라’가 만나 서로의 삶과 예술을 완성해가는 이야기죠.  \n",
      "웃음과 눈물, 역사와 상상이 뒤섞인 로맨스이면서 동시에 ‘연극이 가진 힘’에 대한 찬사이기도 합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "movie = chain1.invoke({\"genre\": \"Drama\"})  # 영화 제목 얻기\n",
    "\n",
    "print(type(movie))\n",
    "print(\" ===> 추천된 영화:\")  # movie 값 출력\n",
    "print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16718b76-f59d-48f7-906f-5d2371417803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "16718b76-f59d-48f7-906f-5d2371417803",
    "outputId": "6e3371cd-d294-4be7-a868-2eae5ee6e5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  movie: ChatPromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, template='{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.'), additional_kwargs={})])\n",
      "         | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B0B0D47890>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B0B0E7B0E0>, root_client=<openai.OpenAI object at 0x000001B0B0B133B0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B0B0D47C80>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "         | StrOutputParser()\n",
      "} middle=[ChatPromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, template='{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B0B0D47890>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B0B0E7B0E0>, root_client=<openai.OpenAI object at 0x000001B0B0B133B0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B0B0D47C80>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n",
      "\n",
      "🔹 영화 줄거리 요약:\n",
      " **버닝 (Burning, 2018)**\n",
      "\n",
      "버닝  \n",
      "감독: 이창동  \n",
      "캐스팅: 유아인, 스티븐 연, 전종서  \n",
      "줄거리: 배달원 종수는 우연히 만난 헤라는 소녀와 친해지지만, 그녀가 아프리카 여행에서 돌아온 뒤 알게 된 신비로운 남자 벤과 함께 삼각 관계가 된다. 어느 날 헤라가 사라지고, 종수는 범인을 찾아 나서면서 계층 사이의 깊은 불신과 분노가 폭발하는데…\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain2)\n",
    "\n",
    "# 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\\n\", response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb684fd",
   "metadata": {},
   "source": [
    "##### chain1과 chain2에서 영화 제목이 일관되게 전달 되도록 변경 ( 잘 동작하는 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e30883ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 영화 줄거리 요약:\n",
      "('**무간도 (無間道)**  \\n'\n",
      " '제목: 무간도(無間道) / Infernal Affairs  \\n'\n",
      " '감독: 유위강(Andrew Lau), 매이초성(Alan Mak)  \\n'\n",
      " '출연: 유덕화(유건명·갱스터), 양조위(진영·경찰), 유가수(마돈나), 임가신(박사), 황추생(황지)  \\n'\n",
      " '줄거리: 1990년대 홍콩, 삼합회 ‘홍싱’의 두목 형님 ‘사오’는 경찰학교 신입생 진영(양조위)을 자신의 조직에 잠입시킨다. 동시에 '\n",
      " '경찰학교 우수생 유건(유덕화)은 졸업과 함께 삼합회에 잠입하라는 비밀 임무를 받는다. 10년 뒤, 진영은 형사과 정보팀 수석 형사가 되어 '\n",
      " '홍싱의 비밀 정보를 흘려주고, 유건은 사오의 최측근이 되어 경찰의 대형 작전을 사전에 빼돌린다. 각자 조직 내 ‘내통자’ 색출 작전이 '\n",
      " '시작되면서, 서로의 존재를 파악하려는 두 남자는 정체와 신념, 그리고 자신이 누구인지조차 점점 흐려져 간다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: \"Drama\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\")\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mylangchain-app-auRXAaAq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
